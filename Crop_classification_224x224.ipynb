{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Crop_classification_224x224.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "f91b3649",
        "0d1e7ed1"
      ]
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f91b3649"
      },
      "source": [
        "### Larger Image size test"
      ],
      "id": "f91b3649"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xuq4VAsq6VTQ",
        "outputId": "2b5fc989-f638-4d35-e0aa-d5f2194d4781"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "Xuq4VAsq6VTQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPrWaF4ZQJ7b"
      },
      "source": [
        "!unzip \"/content/drive/MyDrive/Colab_Files/Crop_disease/Data.zip\" -d \"/content\""
      ],
      "id": "IPrWaF4ZQJ7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f977b821"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from numpy import savez_compressed, load\n",
        "\n",
        "%matplotlib inline"
      ],
      "id": "f977b821",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eda9e95f"
      },
      "source": [
        "from tensorflow.keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPool2D,BatchNormalization,AveragePooling2D,GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model,Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, Callback"
      ],
      "id": "eda9e95f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "id": "bac3e336",
        "outputId": "d2bbe4c0-c23b-4e71-d85b-a61c467f410c"
      },
      "source": [
        "## Simple CNN v4\n",
        "model_2d = tf.keras.Sequential([\n",
        "    # Conv2D(128,(3,3), activation = 'relu'),\n",
        "    # MaxPool2D(2,2),\n",
        "    # BatchNormalization(),\n",
        "    # Dropout(0.2),\n",
        "    Conv2D(128,(3,3), activation='relu', input_shape = (256,256,3)),\n",
        "    MaxPool2D(2,2),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Conv2D(64,(3,3), activation='relu'),\n",
        "    MaxPool2D(2,2),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "    Conv2D(16,(3,3), activation='relu'),\n",
        "    MaxPool2D(2,2),\n",
        "    BatchNormalization(),\n",
        "    Flatten(),\n",
        "    Dropout(0.2),\n",
        "    Dense(128, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(15, activation='softmax')\n",
        "])\n",
        "opt_2d = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model_2d.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "#     tf.keras.applications.vgg16.preprocess_input\n",
        "# )\n",
        "\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "    directory = '/content/train/', class_mode= 'categorical', batch_size=32\n",
        ")\n",
        "val_set = val_datagen.flow_from_directory(\n",
        "    directory='/content/val/', class_mode= 'categorical', batch_size=32\n",
        ")\n",
        "\n",
        "\n",
        "history_2d = model_2d.fit(train_set, validation_data=val_set, epochs=15)\n",
        "\n",
        "####\n",
        "acc = history_2d.history['accuracy']\n",
        "val_acc = history_2d.history['val_accuracy']\n",
        "loss = history_2d.history['loss']\n",
        "val_loss = history_2d.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "plt.show()"
      ],
      "id": "bac3e336",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12376 images belonging to 15 classes.\n",
            "Found 4138 images belonging to 15 classes.\n",
            "Epoch 1/15\n",
            "387/387 [==============================] - 145s 365ms/step - loss: 0.8004 - accuracy: 0.7483 - val_loss: 3.9378 - val_accuracy: 0.3731\n",
            "Epoch 2/15\n",
            "387/387 [==============================] - 138s 357ms/step - loss: 0.3838 - accuracy: 0.8767 - val_loss: 2.1379 - val_accuracy: 0.5176\n",
            "Epoch 3/15\n",
            "387/387 [==============================] - 139s 358ms/step - loss: 0.2395 - accuracy: 0.9261 - val_loss: 0.8046 - val_accuracy: 0.7395\n",
            "Epoch 4/15\n",
            "387/387 [==============================] - 139s 358ms/step - loss: 0.1767 - accuracy: 0.9437 - val_loss: 1.4622 - val_accuracy: 0.6160\n",
            "Epoch 5/15\n",
            "387/387 [==============================] - 139s 358ms/step - loss: 0.1257 - accuracy: 0.9615 - val_loss: 1.5573 - val_accuracy: 0.6593\n",
            "Epoch 6/15\n",
            "387/387 [==============================] - 138s 358ms/step - loss: 0.0858 - accuracy: 0.9756 - val_loss: 0.4854 - val_accuracy: 0.8639\n",
            "Epoch 7/15\n",
            "387/387 [==============================] - 138s 357ms/step - loss: 0.0870 - accuracy: 0.9730 - val_loss: 1.0561 - val_accuracy: 0.7460\n",
            "Epoch 8/15\n",
            "387/387 [==============================] - 137s 354ms/step - loss: 0.0747 - accuracy: 0.9757 - val_loss: 0.8956 - val_accuracy: 0.7617\n",
            "Epoch 9/15\n",
            "387/387 [==============================] - 138s 356ms/step - loss: 0.0599 - accuracy: 0.9818 - val_loss: 0.5021 - val_accuracy: 0.8584\n",
            "Epoch 10/15\n",
            "387/387 [==============================] - 138s 356ms/step - loss: 0.0582 - accuracy: 0.9827 - val_loss: 0.4273 - val_accuracy: 0.8804\n",
            "Epoch 11/15\n",
            "387/387 [==============================] - 138s 355ms/step - loss: 0.0500 - accuracy: 0.9855 - val_loss: 0.7129 - val_accuracy: 0.8062\n",
            "Epoch 12/15\n",
            "387/387 [==============================] - 138s 355ms/step - loss: 0.0472 - accuracy: 0.9848 - val_loss: 0.7770 - val_accuracy: 0.7994\n",
            "Epoch 13/15\n",
            "387/387 [==============================] - 137s 355ms/step - loss: 0.0404 - accuracy: 0.9871 - val_loss: 0.5880 - val_accuracy: 0.8494\n",
            "Epoch 14/15\n",
            "387/387 [==============================] - 138s 355ms/step - loss: 0.0337 - accuracy: 0.9905 - val_loss: 0.8035 - val_accuracy: 0.8033\n",
            "Epoch 15/15\n",
            "387/387 [==============================] - 137s 355ms/step - loss: 0.0357 - accuracy: 0.9888 - val_loss: 0.9922 - val_accuracy: 0.7600\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00eea7b4"
      },
      "source": [
        ""
      ],
      "id": "00eea7b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2b124b1"
      },
      "source": [
        ""
      ],
      "id": "a2b124b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c418a2e2"
      },
      "source": [
        ""
      ],
      "id": "c418a2e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d1e7ed1"
      },
      "source": [
        "### VGG16 with image augmentation | Summary: Train: 99.9%; Val: 94.3%"
      ],
      "id": "0d1e7ed1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acc1d5a6",
        "outputId": "52d34c86-0564-489a-fa99-16ef0ec0ac15"
      },
      "source": [
        "VGG = tf.keras.applications.VGG16(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(256,256,3),\n",
        ")\n",
        "# VGG.summary()\n",
        "for layer in VGG.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_vgg = Sequential()\n",
        "model_vgg.add(VGG)\n",
        "model_vgg.add(tf.keras.layers.Flatten())\n",
        "model_vgg.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
        "model_vgg.add(BatchNormalization())\n",
        "model_vgg.add(tf.keras.layers.Dropout(0.2))\n",
        "model_vgg.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
        "model_vgg.add(BatchNormalization())\n",
        "# model_vgg.add(tf.keras.layers.Dropout(0.4))\n",
        "model_vgg.add(tf.keras.layers.Dense(15, activation = 'softmax'))\n",
        "\n",
        "model_vgg.summary()\n",
        "# optimizer1 = tf.keras.optimizers.SGD(lr=0.05, decay=0.1, momentum=0.1, nesterov=False)\n",
        "\n",
        "# model_vgg.compile(optimizer= optimizer1, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# history_vgg = model_vgg.fit(X_train,Y_train, validation_data=(X_val, Y_val), batch_size = 50, epochs=20)\n",
        "\n",
        "model_vgg.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "#     rotation_range=40,\n",
        "# \t\tzoom_range=0.2,\n",
        "# \t\twidth_shift_range=0.2,\n",
        "# \t\theight_shift_range=0.2,\n",
        "# \t\tshear_range=0.2,\n",
        "# \t\thorizontal_flip=True,\n",
        "# \t\tfill_mode=\"nearest\"\n",
        "# )\n",
        "\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "    directory = '/content/train/', class_mode= 'categorical', batch_size=64\n",
        ")\n",
        "val_set = val_datagen.flow_from_directory(\n",
        "    directory='/content/val/', class_mode= 'categorical', batch_size=64\n",
        ")\n",
        "\n",
        "\n",
        "history_vgg = model_vgg.fit(train_set, validation_data=val_set, epochs=5)"
      ],
      "id": "acc1d5a6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 8, 8, 512)         14714688  \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 128)               4194432   \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 128)              512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 64)               256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 15)                975       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,919,119\n",
            "Trainable params: 4,204,047\n",
            "Non-trainable params: 14,715,072\n",
            "_________________________________________________________________\n",
            "Found 12376 images belonging to 15 classes.\n",
            "Found 4138 images belonging to 15 classes.\n",
            "Epoch 1/5\n",
            "194/194 [==============================] - 150s 764ms/step - loss: 0.5352 - accuracy: 0.8464 - val_loss: 0.2499 - val_accuracy: 0.9207\n",
            "Epoch 2/5\n",
            "194/194 [==============================] - 148s 762ms/step - loss: 0.0901 - accuracy: 0.9809 - val_loss: 0.1982 - val_accuracy: 0.9377\n",
            "Epoch 3/5\n",
            "194/194 [==============================] - 148s 762ms/step - loss: 0.0283 - accuracy: 0.9971 - val_loss: 0.1667 - val_accuracy: 0.9476\n",
            "Epoch 4/5\n",
            "194/194 [==============================] - 148s 762ms/step - loss: 0.0142 - accuracy: 0.9987 - val_loss: 0.1646 - val_accuracy: 0.9454\n",
            "Epoch 5/5\n",
            "194/194 [==============================] - 148s 764ms/step - loss: 0.0101 - accuracy: 0.9989 - val_loss: 0.1846 - val_accuracy: 0.9427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4350c46"
      },
      "source": [
        "### **With preprocessing inputs** | Summary: Train: 99.9%; Val: 96.33%\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "id": "f4350c46"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eda4bed",
        "outputId": "cbf382ff-f70e-4f56-9d96-4851f1e0909f"
      },
      "source": [
        "VGG = tf.keras.applications.VGG16(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(256,256,3),\n",
        ")\n",
        "# VGG.summary()\n",
        "for layer in VGG.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_vgg = Sequential()\n",
        "model_vgg.add(VGG)\n",
        "model_vgg.add(tf.keras.layers.Flatten())\n",
        "model_vgg.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
        "model_vgg.add(BatchNormalization())\n",
        "model_vgg.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
        "model_vgg.add(BatchNormalization())\n",
        "model_vgg.add(tf.keras.layers.Dense(15, activation = 'softmax'))\n",
        "\n",
        "model_vgg.summary()\n",
        "# optimizer1 = tf.keras.optimizers.SGD(lr=0.05, decay=0.1, momentum=0.1, nesterov=False)\n",
        "\n",
        "# model_vgg.compile(optimizer= optimizer1, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# history_vgg = model_vgg.fit(X_train,Y_train, validation_data=(X_val, Y_val), batch_size = 50, epochs=20)\n",
        "\n",
        "model_vgg.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    tf.keras.applications.vgg16.preprocess_input\n",
        ")\n",
        "\n",
        "# train_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    tf.keras.applications.vgg16.preprocess_input\n",
        ")\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "    directory = '/content/train/', class_mode= 'categorical', batch_size=64\n",
        ")\n",
        "val_set = val_datagen.flow_from_directory(\n",
        "    directory='/content/val/', class_mode= 'categorical', batch_size=64\n",
        ")\n",
        "\n",
        "\n",
        "history_vgg = model_vgg.fit(train_set, validation_data=val_set, epochs=10)"
      ],
      "id": "8eda4bed",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "58900480/58889256 [==============================] - 1s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 8, 8, 512)         14714688  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32768)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               4194432   \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 128)              512       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 64)               256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 15)                975       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,919,119\n",
            "Trainable params: 4,204,047\n",
            "Non-trainable params: 14,715,072\n",
            "_________________________________________________________________\n",
            "Found 12376 images belonging to 15 classes.\n",
            "Found 4138 images belonging to 15 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "194/194 [==============================] - 193s 863ms/step - loss: 0.4855 - accuracy: 0.8659 - val_loss: 0.2629 - val_accuracy: 0.9193\n",
            "Epoch 2/10\n",
            "194/194 [==============================] - 142s 733ms/step - loss: 0.0550 - accuracy: 0.9914 - val_loss: 0.1572 - val_accuracy: 0.9531\n",
            "Epoch 3/10\n",
            "194/194 [==============================] - 142s 734ms/step - loss: 0.0129 - accuracy: 0.9993 - val_loss: 0.1389 - val_accuracy: 0.9599\n",
            "Epoch 4/10\n",
            "194/194 [==============================] - 142s 734ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.1371 - val_accuracy: 0.9601\n",
            "Epoch 5/10\n",
            "194/194 [==============================] - 142s 733ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.1315 - val_accuracy: 0.9621\n",
            "Epoch 6/10\n",
            "194/194 [==============================] - 142s 733ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1272 - val_accuracy: 0.9618\n",
            "Epoch 7/10\n",
            "194/194 [==============================] - 142s 733ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1313 - val_accuracy: 0.9599\n",
            "Epoch 8/10\n",
            "194/194 [==============================] - 142s 733ms/step - loss: 9.0883e-04 - accuracy: 1.0000 - val_loss: 0.1309 - val_accuracy: 0.9618\n",
            "Epoch 9/10\n",
            "194/194 [==============================] - 142s 733ms/step - loss: 6.6334e-04 - accuracy: 1.0000 - val_loss: 0.1317 - val_accuracy: 0.9625\n",
            "Epoch 10/10\n",
            "194/194 [==============================] - 142s 733ms/step - loss: 5.3631e-04 - accuracy: 1.0000 - val_loss: 0.1285 - val_accuracy: 0.9633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMFG7JmZLX-4"
      },
      "source": [
        "### Data Augmentation | Summary: Train: 96.3%; Val: 95%"
      ],
      "id": "GMFG7JmZLX-4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqJW_UK0E6nu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84848f8e-9b81-4668-e3e4-39517f35c149"
      },
      "source": [
        "VGG = tf.keras.applications.VGG16(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(256,256,3),\n",
        ")\n",
        "# VGG.summary()\n",
        "for layer in VGG.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_vgg = Sequential()\n",
        "model_vgg.add(VGG)\n",
        "model_vgg.add(tf.keras.layers.Flatten())\n",
        "model_vgg.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
        "model_vgg.add(BatchNormalization())\n",
        "model_vgg.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
        "model_vgg.add(BatchNormalization())\n",
        "model_vgg.add(tf.keras.layers.Dense(15, activation = 'softmax'))\n",
        "\n",
        "model_vgg.summary()\n",
        "# optimizer1 = tf.keras.optimizers.SGD(lr=0.05, decay=0.1, momentum=0.1, nesterov=False)\n",
        "\n",
        "# model_vgg.compile(optimizer= optimizer1, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# history_vgg = model_vgg.fit(X_train,Y_train, validation_data=(X_val, Y_val), batch_size = 50, epochs=20)\n",
        "\n",
        "model_vgg.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "\t\tzoom_range=0.1,\n",
        "\t\twidth_shift_range=0.1,\n",
        "\t\theight_shift_range=0.1,\n",
        "\t\tshear_range=0.2,\n",
        "\t\thorizontal_flip=True,\n",
        "\t\tfill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "# train_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "    directory = '/content/train/', class_mode= 'categorical', batch_size=64\n",
        ")\n",
        "val_set = val_datagen.flow_from_directory(\n",
        "    directory='/content/val/', class_mode= 'categorical', batch_size=64\n",
        ")\n",
        "\n",
        "\n",
        "history_vgg = model_vgg.fit(train_set, validation_data=val_set, epochs=10)"
      ],
      "id": "TqJW_UK0E6nu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 8, 8, 512)         14714688  \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 128)               4194432   \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 128)              512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 64)               256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 15)                975       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,919,119\n",
            "Trainable params: 4,204,047\n",
            "Non-trainable params: 14,715,072\n",
            "_________________________________________________________________\n",
            "Found 12376 images belonging to 15 classes.\n",
            "Found 4138 images belonging to 15 classes.\n",
            "Epoch 1/10\n",
            "194/194 [==============================] - 258s 1s/step - loss: 0.7245 - accuracy: 0.7902 - val_loss: 0.4379 - val_accuracy: 0.8591\n",
            "Epoch 2/10\n",
            "194/194 [==============================] - 256s 1s/step - loss: 0.3130 - accuracy: 0.9034 - val_loss: 0.2961 - val_accuracy: 0.9036\n",
            "Epoch 3/10\n",
            "194/194 [==============================] - 264s 1s/step - loss: 0.2270 - accuracy: 0.9283 - val_loss: 0.2415 - val_accuracy: 0.9210\n",
            "Epoch 4/10\n",
            "194/194 [==============================] - 265s 1s/step - loss: 0.1924 - accuracy: 0.9375 - val_loss: 0.2389 - val_accuracy: 0.9181\n",
            "Epoch 5/10\n",
            "194/194 [==============================] - 264s 1s/step - loss: 0.1680 - accuracy: 0.9465 - val_loss: 0.2090 - val_accuracy: 0.9285\n",
            "Epoch 6/10\n",
            "194/194 [==============================] - 265s 1s/step - loss: 0.1561 - accuracy: 0.9476 - val_loss: 0.2289 - val_accuracy: 0.9273\n",
            "Epoch 7/10\n",
            "194/194 [==============================] - 262s 1s/step - loss: 0.1420 - accuracy: 0.9507 - val_loss: 0.2029 - val_accuracy: 0.9331\n",
            "Epoch 8/10\n",
            "194/194 [==============================] - 264s 1s/step - loss: 0.1195 - accuracy: 0.9613 - val_loss: 0.1562 - val_accuracy: 0.9500\n",
            "Epoch 9/10\n",
            "194/194 [==============================] - 265s 1s/step - loss: 0.1216 - accuracy: 0.9608 - val_loss: 0.1824 - val_accuracy: 0.9357\n",
            "Epoch 10/10\n",
            "194/194 [==============================] - 264s 1s/step - loss: 0.1101 - accuracy: 0.9631 - val_loss: 0.1817 - val_accuracy: 0.9420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyalyCTnda8C",
        "outputId": "e85e2261-53d3-4922-bce3-938c7c88ef92"
      },
      "source": [
        "VGG = tf.keras.applications.VGG16(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(256,256,3),\n",
        ")\n",
        "# VGG.summary()\n",
        "for layer in VGG.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_vgg = Sequential()\n",
        "model_vgg.add(VGG)\n",
        "model_vgg.add(tf.keras.layers.Flatten())\n",
        "model_vgg.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
        "model_vgg.add(BatchNormalization())\n",
        "model_vgg.add(tf.keras.layers.Dropout(0.4))\n",
        "model_vgg.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
        "model_vgg.add(BatchNormalization())\n",
        "model_vgg.add(tf.keras.layers.Dropout(0.1))\n",
        "model_vgg.add(tf.keras.layers.Dense(15, activation = 'softmax'))\n",
        "\n",
        "model_vgg.summary()\n",
        "# optimizer1 = tf.keras.optimizers.SGD(lr=0.05, decay=0.1, momentum=0.1, nesterov=False)\n",
        "\n",
        "# model_vgg.compile(optimizer= optimizer1, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# history_vgg = model_vgg.fit(X_train,Y_train, validation_data=(X_val, Y_val), batch_size = 50, epochs=20)\n",
        "\n",
        "model_vgg.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "\t\tzoom_range=0.1,\n",
        "\t\twidth_shift_range=0.1,\n",
        "\t\theight_shift_range=0.1,\n",
        "\t\tshear_range=0.2,\n",
        "\t\thorizontal_flip=True,\n",
        "\t\tfill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "# train_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "    directory = '/content/train/', class_mode= 'categorical', batch_size=64\n",
        ")\n",
        "val_set = val_datagen.flow_from_directory(\n",
        "    directory='/content/val/', class_mode= 'categorical', batch_size=64\n",
        ")\n",
        "\n",
        "\n",
        "history_vgg = model_vgg.fit(train_set, validation_data=val_set, epochs=15)"
      ],
      "id": "zyalyCTnda8C",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 8, 8, 512)         14714688  \n",
            "                                                                 \n",
            " flatten_17 (Flatten)        (None, 32768)             0         \n",
            "                                                                 \n",
            " dense_48 (Dense)            (None, 128)               4194432   \n",
            "                                                                 \n",
            " batch_normalization_22 (Bat  (None, 128)              512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_23 (Bat  (None, 64)               256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_50 (Dense)            (None, 15)                975       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,919,119\n",
            "Trainable params: 4,204,047\n",
            "Non-trainable params: 14,715,072\n",
            "_________________________________________________________________\n",
            "Found 12376 images belonging to 15 classes.\n",
            "Found 4138 images belonging to 15 classes.\n",
            "Epoch 1/15\n",
            "194/194 [==============================] - 265s 1s/step - loss: 0.9874 - accuracy: 0.6963 - val_loss: 0.4947 - val_accuracy: 0.8395\n",
            "Epoch 2/15\n",
            "194/194 [==============================] - 259s 1s/step - loss: 0.5009 - accuracy: 0.8423 - val_loss: 0.3205 - val_accuracy: 0.8968\n",
            "Epoch 3/15\n",
            "194/194 [==============================] - 259s 1s/step - loss: 0.3885 - accuracy: 0.8740 - val_loss: 0.2680 - val_accuracy: 0.9145\n",
            "Epoch 4/15\n",
            "194/194 [==============================] - 261s 1s/step - loss: 0.3251 - accuracy: 0.8954 - val_loss: 0.2091 - val_accuracy: 0.9294\n",
            "Epoch 5/15\n",
            "194/194 [==============================] - 259s 1s/step - loss: 0.2865 - accuracy: 0.9052 - val_loss: 0.2420 - val_accuracy: 0.9207\n",
            "Epoch 6/15\n",
            "194/194 [==============================] - 257s 1s/step - loss: 0.2705 - accuracy: 0.9133 - val_loss: 0.2296 - val_accuracy: 0.9227\n",
            "Epoch 7/15\n",
            "194/194 [==============================] - 258s 1s/step - loss: 0.2555 - accuracy: 0.9144 - val_loss: 0.1841 - val_accuracy: 0.9386\n",
            "Epoch 8/15\n",
            "194/194 [==============================] - 257s 1s/step - loss: 0.2389 - accuracy: 0.9200 - val_loss: 0.2226 - val_accuracy: 0.9261\n",
            "Epoch 9/15\n",
            "194/194 [==============================] - 256s 1s/step - loss: 0.2213 - accuracy: 0.9273 - val_loss: 0.2484 - val_accuracy: 0.9188\n",
            "Epoch 10/15\n",
            "194/194 [==============================] - 257s 1s/step - loss: 0.2111 - accuracy: 0.9303 - val_loss: 0.2289 - val_accuracy: 0.9292\n",
            "Epoch 11/15\n",
            "194/194 [==============================] - 256s 1s/step - loss: 0.1997 - accuracy: 0.9342 - val_loss: 0.1929 - val_accuracy: 0.9372\n",
            "Epoch 12/15\n",
            "194/194 [==============================] - 255s 1s/step - loss: 0.1880 - accuracy: 0.9353 - val_loss: 0.2280 - val_accuracy: 0.9248\n",
            "Epoch 13/15\n",
            "194/194 [==============================] - 259s 1s/step - loss: 0.1873 - accuracy: 0.9376 - val_loss: 0.1747 - val_accuracy: 0.9432\n",
            "Epoch 14/15\n",
            "194/194 [==============================] - 262s 1s/step - loss: 0.1698 - accuracy: 0.9442 - val_loss: 0.1964 - val_accuracy: 0.9437\n",
            "Epoch 15/15\n",
            "194/194 [==============================] - 261s 1s/step - loss: 0.1737 - accuracy: 0.9401 - val_loss: 0.1799 - val_accuracy: 0.9393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b30aad2"
      },
      "source": [
        "### MobilenetV2 with image augmentation | Summary: Train: 94%; Val: 84%"
      ],
      "id": "6b30aad2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c765991",
        "outputId": "a5be3237-3850-4b44-e479-16dd24446546"
      },
      "source": [
        "mbnet_v2 = tf.keras.applications.MobileNetV2(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_shape=(256,256,3)\n",
        ")\n",
        "\n",
        "mbnet_v2.trainable = False\n",
        "model_mbnet = Sequential()\n",
        "model_mbnet.add(mbnet_v2)\n",
        "model_mbnet.add(tf.keras.layers.Flatten())\n",
        "model_mbnet.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
        "model_mbnet.add(BatchNormalization())\n",
        "# model_mbnet.add(tf.keras.layers.Dropout(0.4))\n",
        "model_mbnet.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
        "model_mbnet.add(BatchNormalization())\n",
        "# model_mbnet.add(tf.keras.layers.Dropout(0.1))\n",
        "model_mbnet.add(tf.keras.layers.Dense(15, activation = 'softmax'))\n",
        "\n",
        "model_mbnet.summary()\n",
        "model_mbnet.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# early_stopping_cb = Callback.EarlyStopping(monitor=\"loss\", patience=3)\n",
        "\n",
        "# train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "#     tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "# )\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(tf.keras.applications.MobileNetV2.preprocess_input)\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(tf.keras.applications.MobileNetV2.preprocess_input)\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "    directory = '/content/train/', class_mode= 'categorical', batch_size=64\n",
        ")\n",
        "val_set = val_datagen.flow_from_directory(\n",
        "    directory='/content/val/', class_mode= 'categorical', batch_size=64\n",
        ")\n",
        "\n",
        "history_mbnet = model_mbnet.fit(train_set, validation_data=val_set, epochs=5)"
      ],
      "id": "4c765991",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " mobilenetv2_1.00_224 (Funct  (None, 8, 8, 1280)       2257984   \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " flatten_12 (Flatten)        (None, 81920)             0         \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 128)               10485888  \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 128)              512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 64)               256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 15)                975       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,753,871\n",
            "Trainable params: 10,495,503\n",
            "Non-trainable params: 2,258,368\n",
            "_________________________________________________________________\n",
            "Found 12376 images belonging to 15 classes.\n",
            "Found 4138 images belonging to 15 classes.\n",
            "Epoch 1/5\n",
            "194/194 [==============================] - 59s 285ms/step - loss: 1.1670 - accuracy: 0.6427 - val_loss: 0.7024 - val_accuracy: 0.7832\n",
            "Epoch 2/5\n",
            "194/194 [==============================] - 53s 274ms/step - loss: 0.5745 - accuracy: 0.8201 - val_loss: 0.6010 - val_accuracy: 0.8040\n",
            "Epoch 3/5\n",
            "194/194 [==============================] - 53s 273ms/step - loss: 0.3584 - accuracy: 0.8889 - val_loss: 0.5037 - val_accuracy: 0.8388\n",
            "Epoch 4/5\n",
            "194/194 [==============================] - 53s 273ms/step - loss: 0.2536 - accuracy: 0.9233 - val_loss: 0.5762 - val_accuracy: 0.8304\n",
            "Epoch 5/5\n",
            "194/194 [==============================] - 53s 273ms/step - loss: 0.1863 - accuracy: 0.9418 - val_loss: 0.6743 - val_accuracy: 0.8026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cd5f364"
      },
      "source": [
        "**With preprocessing step**"
      ],
      "id": "6cd5f364"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53ba5334",
        "outputId": "425ad943-ab52-4d3d-9bd9-5a4d3affa72a"
      },
      "source": [
        "mbnet_v2 = tf.keras.applications.MobileNetV2(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_shape=(256,256,3)\n",
        ")\n",
        "\n",
        "mbnet_v2.trainable = False\n",
        "model_mbnet = Sequential()\n",
        "model_mbnet.add(mbnet_v2)\n",
        "model_mbnet.add(tf.keras.layers.Flatten())\n",
        "model_mbnet.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
        "model_mbnet.add(BatchNormalization())\n",
        "model_mbnet.add(tf.keras.layers.Dropout(0.4))\n",
        "model_mbnet.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
        "model_mbnet.add(BatchNormalization())\n",
        "model_mbnet.add(tf.keras.layers.Dropout(0.1))\n",
        "model_mbnet.add(tf.keras.layers.Dense(15, activation = 'softmax'))\n",
        "\n",
        "model_mbnet.summary()\n",
        "model_mbnet.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# early_stopping_cb = Callback.EarlyStopping(monitor=\"loss\", patience=3)\n",
        "\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "\t\tzoom_range=0.1,\n",
        "\t\twidth_shift_range=0.1,\n",
        "\t\theight_shift_range=0.1,\n",
        "\t\tshear_range=0.2,\n",
        "\t\thorizontal_flip=True,\n",
        "\t\tfill_mode=\"nearest\"\n",
        ")\n",
        "# train_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "train_set = train_datagen.flow_from_directory(\n",
        "    directory = '/content/train/', class_mode= 'categorical', batch_size=64\n",
        ")\n",
        "val_set = val_datagen.flow_from_directory(\n",
        "    directory='/content/val/', class_mode= 'categorical', batch_size=64\n",
        ")\n",
        "\n",
        "history_mbnet = model_mbnet.fit(train_set, validation_data=val_set, epochs=15)"
      ],
      "id": "53ba5334",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " mobilenetv2_1.00_224 (Funct  (None, 8, 8, 1280)       2257984   \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " flatten_14 (Flatten)        (None, 81920)             0         \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 128)               10485888  \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  (None, 128)              512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_21 (Bat  (None, 64)               256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 15)                975       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,753,871\n",
            "Trainable params: 10,495,503\n",
            "Non-trainable params: 2,258,368\n",
            "_________________________________________________________________\n",
            "Found 12376 images belonging to 15 classes.\n",
            "Found 4138 images belonging to 15 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "194/194 [==============================] - 58s 282ms/step - loss: 1.1600 - accuracy: 0.6479 - val_loss: 0.6740 - val_accuracy: 0.7885\n",
            "Epoch 2/5\n",
            "194/194 [==============================] - 53s 275ms/step - loss: 0.5608 - accuracy: 0.8250 - val_loss: 0.6476 - val_accuracy: 0.7941\n",
            "Epoch 3/5\n",
            "194/194 [==============================] - 53s 274ms/step - loss: 0.3630 - accuracy: 0.8874 - val_loss: 0.6068 - val_accuracy: 0.8006\n",
            "Epoch 4/5\n",
            "194/194 [==============================] - 53s 275ms/step - loss: 0.2606 - accuracy: 0.9167 - val_loss: 0.5255 - val_accuracy: 0.8330\n",
            "Epoch 5/5\n",
            "194/194 [==============================] - 53s 274ms/step - loss: 0.1706 - accuracy: 0.9476 - val_loss: 0.5519 - val_accuracy: 0.8415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "177ad98d"
      },
      "source": [
        ""
      ],
      "id": "177ad98d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3YOKCZlL5fd"
      },
      "source": [
        "### EfficientNet | Summary: Train: 91.6%; Val: 92.3%"
      ],
      "id": "d3YOKCZlL5fd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f243bda",
        "outputId": "37e3e1ed-e288-43f0-f5dd-ec2c7057efbc"
      },
      "source": [
        "# import tensorflow_hub as hub\n",
        "old_model_2 = tf.keras.applications.efficientnet.EfficientNetB7(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_shape=(256,256,3)\n",
        ")\n",
        "# efficientnet = 'efficientnet/'\n",
        "# old_model_2.trainable = False\n",
        "for layer in old_model_2.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_3b = Sequential()\n",
        "model_3b.add(old_model_2)\n",
        "model_3b.add(tf.keras.layers.Flatten())\n",
        "model_3b.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
        "model_3b.add(BatchNormalization())\n",
        "model_3b.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
        "model_3b.add(BatchNormalization())\n",
        "model_3b.add(tf.keras.layers.Dense(15, activation = 'softmax'))\n",
        "\n",
        "model_3b.summary()\n",
        "# optimizer = tf.keras.optimizers.SGD(lr=0.01, decay=0.1, momentum=0.1, nesterov=False)\n",
        "model_3b.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "\t\tzoom_range=0.1,\n",
        "\t\twidth_shift_range=0.1,\n",
        "\t\theight_shift_range=0.1,\n",
        "\t\tshear_range=0.2,\n",
        "\t\thorizontal_flip=True,\n",
        "\t\tfill_mode=\"nearest\"\n",
        ")\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "train_set = train_datagen.flow_from_directory(directory = '/content/train/', class_mode= 'categorical', batch_size=32)\n",
        "val_set = val_datagen.flow_from_directory(directory='/content/val/', class_mode= 'categorical', batch_size=32)\n",
        "\n",
        "history_3b = model_3b.fit(train_set, validation_data=val_set,batch_size = 50, epochs=5)"
      ],
      "id": "0f243bda",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb7_notop.h5\n",
            "258080768/258076736 [==============================] - 3s 0us/step\n",
            "258088960/258076736 [==============================] - 3s 0us/step\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnetb7 (Functional)  (None, 8, 8, 2560)       64097687  \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 163840)            0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               20971648  \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 128)              512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 64)               256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 15)                975       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 85,079,334\n",
            "Trainable params: 20,981,263\n",
            "Non-trainable params: 64,098,071\n",
            "_________________________________________________________________\n",
            "Found 12376 images belonging to 15 classes.\n",
            "Found 4138 images belonging to 15 classes.\n",
            "Epoch 1/5\n",
            "387/387 [==============================] - 529s 1s/step - loss: 0.9010 - accuracy: 0.7241 - val_loss: 0.4589 - val_accuracy: 0.8497\n",
            "Epoch 2/5\n",
            "387/387 [==============================] - 493s 1s/step - loss: 0.4176 - accuracy: 0.8686 - val_loss: 0.3412 - val_accuracy: 0.8876\n",
            "Epoch 3/5\n",
            "387/387 [==============================] - 494s 1s/step - loss: 0.3197 - accuracy: 0.8956 - val_loss: 0.3317 - val_accuracy: 0.8920\n",
            "Epoch 4/5\n",
            "387/387 [==============================] - 493s 1s/step - loss: 0.2762 - accuracy: 0.9071 - val_loss: 0.2413 - val_accuracy: 0.9263\n",
            "Epoch 5/5\n",
            "387/387 [==============================] - 493s 1s/step - loss: 0.2508 - accuracy: 0.9159 - val_loss: 0.2319 - val_accuracy: 0.9227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4eyuhg_VPa0",
        "outputId": "c02fb138-e46e-4d1b-e927-5a3ab4b17b98"
      },
      "source": [
        "# import tensorflow_hub as hub\n",
        "old_model_2 = tf.keras.applications.efficientnet.EfficientNetB7(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_shape=(256,256,3)\n",
        ")\n",
        "# efficientnet = 'efficientnet/'\n",
        "# old_model_2.trainable = False\n",
        "for layer in old_model_2.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_3b = Sequential()\n",
        "model_3b.add(old_model_2)\n",
        "model_3b.add(tf.keras.layers.Flatten())\n",
        "model_3b.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
        "model_3b.add(BatchNormalization())\n",
        "model_3b.add(tf.keras.layers.Dense(64, activation = 'relu'))\n",
        "model_3b.add(BatchNormalization())\n",
        "model_3b.add(tf.keras.layers.Dense(15, activation = 'softmax'))\n",
        "\n",
        "model_3b.summary()\n",
        "# optimizer = tf.keras.optimizers.SGD(lr=0.01, decay=0.1, momentum=0.1, nesterov=False)\n",
        "model_3b.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(tf.keras.applications.efficientnet.EfficientNetB7.preprocess_input)\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(tf.keras.applications.efficientnet.EfficientNetB7.preprocess_input)\n",
        "train_set = train_datagen.flow_from_directory(directory = '/content/train/', class_mode= 'categorical', batch_size=32)\n",
        "val_set = val_datagen.flow_from_directory(directory='/content/val/', class_mode= 'categorical', batch_size=32)\n",
        "\n",
        "history_3b = model_3b.fit(train_set, validation_data=val_set,batch_size = 50, epochs=10)"
      ],
      "id": "e4eyuhg_VPa0",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnetb7 (Functional)  (None, 8, 8, 2560)       64097687  \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 163840)            0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 128)               20971648  \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 15)                975       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 85,078,566\n",
            "Trainable params: 20,980,879\n",
            "Non-trainable params: 64,097,687\n",
            "_________________________________________________________________\n",
            "Found 12376 images belonging to 15 classes.\n",
            "Found 4138 images belonging to 15 classes.\n",
            "Epoch 1/5\n",
            "387/387 [==============================] - 524s 1s/step - loss: 3.9069 - accuracy: 0.5566 - val_loss: 1.8429 - val_accuracy: 0.7170\n",
            "Epoch 2/5\n",
            "387/387 [==============================] - 495s 1s/step - loss: 1.9482 - accuracy: 0.6653 - val_loss: 1.5229 - val_accuracy: 0.7786\n",
            "Epoch 3/5\n",
            "387/387 [==============================] - 495s 1s/step - loss: 1.7451 - accuracy: 0.6860 - val_loss: 1.4723 - val_accuracy: 0.7566\n",
            "Epoch 4/5\n",
            "387/387 [==============================] - 496s 1s/step - loss: 1.6615 - accuracy: 0.6851 - val_loss: 1.4465 - val_accuracy: 0.7620\n",
            "Epoch 5/5\n",
            "387/387 [==============================] - ETA: 0s - loss: 1.6417 - accuracy: 0.6818"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEBy4Av9mbNr"
      },
      "source": [
        ""
      ],
      "id": "cEBy4Av9mbNr",
      "execution_count": null,
      "outputs": []
    }
  ]
}